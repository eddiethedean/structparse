name: Performance Benchmarks

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Create virtual environment
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install maturin pytest

      - name: Build extension module
        run: |
          source .venv/bin/activate
          maturin develop --manifest-path formatparse-pyo3/Cargo.toml --release

      - name: Install test dependencies
        run: |
          source .venv/bin/activate
          pip install pytest-benchmark hypothesis pytest-cov pympler memory_profiler

      - name: Run benchmarks
        continue-on-error: true
        run: |
          source .venv/bin/activate
          pytest tests/test_performance.py --benchmark-only --benchmark-json=benchmark_results.json

      - name: Store benchmark results
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results.json
          retention-days: 30
          if-no-files-found: ignore

      - name: Compare with baseline
        if: always()
        continue-on-error: true
        run: |
          source .venv/bin/activate
          # Only compare if benchmark results exist
          if [ ! -f benchmark_results.json ]; then
            echo "No benchmark results found, skipping comparison."
            exit 0
          fi
          # Check if baseline exists, if not create one
          if [ -f baseline_benchmarks.json ]; then
            echo "Comparing against baseline..."
            python scripts/compare_benchmarks.py
          else
            echo "No baseline found, creating baseline from current results..."
            cp benchmark_results.json baseline_benchmarks.json
          fi

